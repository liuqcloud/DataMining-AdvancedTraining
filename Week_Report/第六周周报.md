# 第六周周报
## 理论学习
这周主要对模型调参方法进行了学习。

 1. 贪心调参

先使用当前对模型影响最大的参数进行调优，达到当前参数下的模型最优化，再使用对模型影响次之的参数进行调优，如此下去，直到所有的参数调整完毕。

这个方法的缺点就是可能会调到局部最优而不是全局最优，但是只需要一步一步的进行参数最优化调试即可，容易理解。需要注意的是在树模型中参数调整的顺序，也就是各个参数对模型的影响程度，日常调参过程中常用的参数和调参顺序：

- ①：max_depth、num_leaves
- ②：min_data_in_leaf、min_child_weight
- ③：bagging_fraction、 feature_fraction、bagging_freq
- ④：reg_lambda、reg_alpha
- ⑤：min_split_gain

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201103110243470.png#pic_center)

可依次将模型的参数通过上面的方式进行调整优化，并且通过可视化观察在每一个最优参数下模型的得分情况
 

 2. 网格搜索

sklearn 提供GridSearchCV用于进行网格搜索，只需要把模型的参数输进去，就能给出最优化的结果和参数。网格搜索是遍历所以的参数组合,从而选出最优的参数组合，相比起贪心调参，网格搜索的结果会更优，但是网格搜索只适合于小数据集，一旦数据的量级上去了，很难得出结果。

 3. 贝叶斯调参

- 贝叶斯调参采用高斯过程，会考虑到之前的参数信息，不断地更新先验；网格搜索则不会考虑先验信息。
- 贝叶斯调参迭代次数少，速度快；网格搜索会遍历所有的可能的参数组合，所以速度慢,参数多时易导致维度爆炸
- 贝叶斯调参针对非凸问题依然稳健；网格搜索针对非凸问题易得到局部最优。


在使用之前需要先安装包bayesian-optimization

贝叶斯调参的主要思想是：利用已有的先验信息去找到使目标函数达到全局最大的参数。给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布）。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。

贝叶斯调参的步骤如下：
- 定义优化函数(rf_cv）
- 建立模型
- 定义待优化的参数
- 得到优化结果，并返回要优化的分数指标

## 实践
用Lightgbm算法调参优化后的结果
![在这里插入图片描述](https://img-blog.csdnimg.cn/20201103112051901.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpdXFpYW5nTFFscQ==,size_16,color_FFFFFF,t_70#pic_center)
