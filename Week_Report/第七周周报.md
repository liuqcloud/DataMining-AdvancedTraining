# 第七周周报
## 理论学习
这周主要对课上介绍的模型融合方法进行了深入学习。
常用的**模型融合**方法有：
- 平均：简单平均法，加权平均法
- 投票：简单投票法，加权投票法
- 综合：排序融合，log融合
- stacking:构建多层模型，并利用预测结果再拟合预测。
stacking 将若干基学习器获得的预测结果，将预测结果作为新的训练集来训练一个学习器。stacking中由于两层使用的数据不同，所以可以避免信息泄露的问题。
- blending：
选取部分数据预测训练得到预测结果作为新特征，带入剩下的数据中预测。由于blending对将数据划分为两个部分，在最后预测时有部分数据信息将被忽略。同时在使用第二层数据时可能会因为第二层数据较少产生过拟合现象。
- boosting（XGBoost模型，LightGBM模型，CatBoost模型）/bagging（随机森林模型）

## 实践
**模型融合Stack**

```python
from sklearn import metrics
import xgboost as xgb
import lightgbm as lgb

def xgb_model(X_train, y_train, X_test, y_test=None):
    X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2)
    train_matrix = xgb.DMatrix(X_train_split , label=y_train_split)
    valid_matrix = xgb.DMatrix(X_val , label=y_val)
    test_matrix = xgb.DMatrix(X_test)

    params = {
        'booster': 'gbtree',
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'gamma': 1,
        'min_child_weight': 1.5,
        'max_depth': 5,
        'lambda': 10,
        'subsample': 0.7,
        'colsample_bytree': 0.7,
        'colsample_bylevel': 0.7,
        'eta': 0.04,
        'tree_method': 'exact',
        'seed': 2020,
        'n_jobs': -1,
        "silent": True,
    }
    watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]
    
    model = xgb.train(params, train_matrix, num_boost_round=50000, evals=watchlist, verbose_eval=200, early_stopping_rounds=200)
    """计算在验证集上的得分"""
    val_pred  = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)
    fpr, tpr, threshold = metrics.roc_curve(y_val, val_pred)
    roc_auc = metrics.auc(fpr, tpr)
    print('调参后xgboost单模型在验证集上的AUC：{}'.format(roc_auc))
    """对测试集进行预测"""
    test_pred = model.predict(test_matrix, ntree_limit=model.best_ntree_limit)
    
    return test_pred
    

def lgb_model(X_train, y_train, X_test, y_test=None):
    X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2)
    train_matrix = lgb.Dataset(X_train_split, label=y_train_split)
    valid_matrix = lgb.Dataset(X_val, label=y_val)
    
    # 调参后的最优参数
    params = {
        'boosting_type': 'gbdt',
        'objective': 'binary',
        'metric': 'auc',
        'learning_rate': 0.01,
        'min_child_weight': 0.32,
        'num_leaves': 14,
        'max_depth': 4,
        'feature_fraction': 0.81,
        'bagging_fraction': 0.61,
        'bagging_freq': 9,
        'min_data_in_leaf': 13,
        'min_split_gain': 0.27,
        'reg_alpha': 9.58,
        'reg_lambda': 4.62,
        'seed': 2020,
        'n_jobs':-1,
        'silent': True,
        'verbose': -1,
    }
    
    model = lgb.train(params, train_matrix, 50000, valid_sets=[train_matrix, valid_matrix], verbose_eval=500, early_stopping_rounds=500)
    """计算在验证集上的得分"""
    val_pred = model.predict(X_val, num_iteration=model.best_iteration)
    fpr, tpr, threshold = metrics.roc_curve(y_val, val_pred)
    roc_auc = metrics.auc(fpr, tpr)
    print('调参后lightgbm单模型在验证集上的AUC：{}'.format(roc_auc))
    """对测试集进行预测"""
    test_pred = model.predict(X_test, num_iteration=model.best_iteration)
    
    return test_pred
```

```python
from heamy.pipeline import ModelsPipeline

pipeline = ModelsPipeline(model_xgb, model_lgb)
pipeline
```

```python
# 构建第一层新特征，其中k默认是5，表示5折交叉验证，full_test=True，对全部训练集进行训练得到基学习器，然后用基学习器对测试集预测得到新特征
stack_ds = pipeline.stack(k=5, seed=111, full_test=True)
```

```python
from sklearn.linear_model import LogisticRegression
# 第二层使用逻辑回归进行stack
LogisticRegression(solver='lbfgs')
stacker = Classifier(dataset=stack_ds, estimator=LogisticRegression, parameters={'solver': 'lbfgs'})
# 测试集的预测结果
test_pred = stacker.predict()
test_pred
```

```python
"""生成提交格式的DataFrame"""
df_result = pd.DataFrame({'id': df_data.loc[df_data['sample']=='test', 'id'].values, 'isDefault': test_pred})
df_result.sort_values(by='id').head(20)
```

```python
"""保存数据用于预测建模"""
df_result.to_csv('dataset/submission_data_stacking_model.csv', encoding='gbk', index=False)
```
## 项目总结
这次贷款违约预测比赛我的最好成绩是0.7289.
![在这里插入图片描述](https://img-blog.csdnimg.cn/20201111201240626.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpdXFpYW5nTFFscQ==,size_16,color_FFFFFF,t_70#pic_center)
数据挖掘比赛的流程：

 - 数据预处理：对数据集中各个特征的意义、类型、取值分布有一个大致的了解，并对数据进行预处理，解决缺失、噪声、冗余、不一致等问题，使数据集能够更好地被模型利用。

- 特征工程：异常值处理；数据分箱；特征交互；特征编码；特征选择。
对于缺失值过多，或者值都一样的特征，可以删除；均方差和箱形图；固定宽度分箱，分位数分箱，卡方分箱…
- 建模和调参：
建模：逻辑回归，决策树，boosting（XGBoost模型，LightGBM模型，CatBoost模型）/bagging（随机森林模型）
调参：贪心调参；网格调参；贝叶斯调参。
- 模型融合阶段：平均；投票；排序融合；log融合；stacking；blending


第七周周报
理论学习
这周主要对课上介绍的模型融合方法进行了深入学习。
常用的模型融合方法有：

平均：简单平均法，加权平均法
投票：简单投票法，加权投票法
综合：排序融合，log融合
stacking:构建多层模型，并利用预测结果再拟合预测。
stacking 将若干基学习器获得的预测结果，将预测结果作为新的训练集来训练一个学习器。stacking中由于两层使用的数据不同，所以可以避免信息泄露的问题。
blending：
选取部分数据预测训练得到预测结果作为新特征，带入剩下的数据中预测。由于blending对将数据划分为两个部分，在最后预测时有部分数据信息将被忽略。同时在使用第二层数据时可能会因为第二层数据较少产生过拟合现象。
boosting（XGBoost模型，LightGBM模型，CatBoost模型）/bagging（随机森林模型）
实践
模型融合Stack

from sklearn import metrics
import xgboost as xgb
import lightgbm as lgb

def xgb_model(X_train, y_train, X_test, y_test=None):
    X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2)
    train_matrix = xgb.DMatrix(X_train_split , label=y_train_split)
    valid_matrix = xgb.DMatrix(X_val , label=y_val)
    test_matrix = xgb.DMatrix(X_test)

    params = {
        'booster': 'gbtree',
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'gamma': 1,
        'min_child_weight': 1.5,
        'max_depth': 5,
        'lambda': 10,
        'subsample': 0.7,
        'colsample_bytree': 0.7,
        'colsample_bylevel': 0.7,
        'eta': 0.04,
        'tree_method': 'exact',
        'seed': 2020,
        'n_jobs': -1,
        "silent": True,
    }
    watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]
    
    model = xgb.train(params, train_matrix, num_boost_round=50000, evals=watchlist, verbose_eval=200, early_stopping_rounds=200)
    """计算在验证集上的得分"""
    val_pred  = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)
    fpr, tpr, threshold = metrics.roc_curve(y_val, val_pred)
    roc_auc = metrics.auc(fpr, tpr)
    print('调参后xgboost单模型在验证集上的AUC：{}'.format(roc_auc))
    """对测试集进行预测"""
    test_pred = model.predict(test_matrix, ntree_limit=model.best_ntree_limit)
    
    return test_pred
    

def lgb_model(X_train, y_train, X_test, y_test=None):
    X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2)
    train_matrix = lgb.Dataset(X_train_split, label=y_train_split)
    valid_matrix = lgb.Dataset(X_val, label=y_val)
    
    # 调参后的最优参数
    params = {
        'boosting_type': 'gbdt',
        'objective': 'binary',
        'metric': 'auc',
        'learning_rate': 0.01,
        'min_child_weight': 0.32,
        'num_leaves': 14,
        'max_depth': 4,
        'feature_fraction': 0.81,
        'bagging_fraction': 0.61,
        'bagging_freq': 9,
        'min_data_in_leaf': 13,
        'min_split_gain': 0.27,
        'reg_alpha': 9.58,
        'reg_lambda': 4.62,
        'seed': 2020,
        'n_jobs':-1,
        'silent': True,
        'verbose': -1,
    }
    
    model = lgb.train(params, train_matrix, 50000, valid_sets=[train_matrix, valid_matrix], verbose_eval=500, early_stopping_rounds=500)
    """计算在验证集上的得分"""
    val_pred = model.predict(X_val, num_iteration=model.best_iteration)
    fpr, tpr, threshold = metrics.roc_curve(y_val, val_pred)
    roc_auc = metrics.auc(fpr, tpr)
    print('调参后lightgbm单模型在验证集上的AUC：{}'.format(roc_auc))
    """对测试集进行预测"""
    test_pred = model.predict(X_test, num_iteration=model.best_iteration)
    
    return test_pred
from heamy.pipeline import ModelsPipeline

pipeline = ModelsPipeline(model_xgb, model_lgb)
pipeline
# 构建第一层新特征，其中k默认是5，表示5折交叉验证，full_test=True，对全部训练集进行训练得到基学习器，然后用基学习器对测试集预测得到新特征
stack_ds = pipeline.stack(k=5, seed=111, full_test=True)
from sklearn.linear_model import LogisticRegression
# 第二层使用逻辑回归进行stack
LogisticRegression(solver='lbfgs')
stacker = Classifier(dataset=stack_ds, estimator=LogisticRegression, parameters={'solver': 'lbfgs'})
# 测试集的预测结果
test_pred = stacker.predict()
test_pred
"""生成提交格式的DataFrame"""
df_result = pd.DataFrame({'id': df_data.loc[df_data['sample']=='test', 'id'].values, 'isDefault': test_pred})
df_result.sort_values(by='id').head(20)
"""保存数据用于预测建模"""
df_result.to_csv('dataset/submission_data_stacking_model.csv', encoding='gbk', index=False)
项目总结
这次贷款违约预测比赛我的最好成绩是0.7289.
在这里插入图片描述
数据挖掘比赛的流程：

数据预处理：对数据集中各个特征的意义、类型、取值分布有一个大致的了解，并对数据进行预处理，解决缺失、噪声、冗余、不一致等问题，使数据集能够更好地被模型利用。

特征工程：异常值处理；数据分箱；特征交互；特征编码；特征选择。
对于缺失值过多，或者值都一样的特征，可以删除；均方差和箱形图；固定宽度分箱，分位数分箱，卡方分箱…

建模和调参：
建模：逻辑回归，决策树，boosting（XGBoost模型，LightGBM模型，CatBoost模型）/bagging（随机森林模型）
调参：贪心调参；网格调参；贝叶斯调参。

模型融合阶段：平均；投票；排序融合；log融合；stacking；blending

Markdown 3852 字数 145 行数 当前行 142, 当前列 43 文章已保存20:22:54HTML 3536 字数 110 段落
