# 第五周周报
## 理论学习
对上周学的特征选择方法及树模型进行了深入了解。
> 特征选择技术可以精简掉无用的特征，以降低最终模型的复杂性，它的最终目的是得到一个简约模型，在不降低预测准确率或对预测准确率影响不大的情况下提高计算速度。特征选择不是为了减少训练时间（实际上，一些技术会增加总体训练时间），而是为了减少模型评分时间。

特征选择的方法：
1. Filter:基于特征间的关系进行筛选
- 方差选择法:先要计算各个特征的方差，然后根据设定的阈值，选择方差大于阈值的特征
- 相关系数法
	Pearson 相关系数 皮尔森相关系数是一种最简单的，可以帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性。 结果的取值区间为 [-1，1] ， -1 表示完全的负相关， +1表示完全的正相关，0 表示没有线性相关。
	距离相关系数 如果距离相关系数为0，我们可以说这两个变量是独立的
- 卡方检验
经典的卡方检验是用于检验自变量对因变量的相关性。 假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距。
- 互信息和最大信息系数(MIC) 
经典的互信息也是评价定性自变量对定性因变量的相关性的，通常变量需要先离散化，而互信息的结果对离散化的方式很敏感。 最大信息系数它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在 [ 0 , 1 ]。
2. Wrapper （RFE）
- 递归特征消除法：递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
#递归特征消除法，返回特征选择后的数据
#参数estimator为基模型
#参数n_features_to_select为选择的特征个数

RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(train,target_train)
```

3. Embedded
- 基于惩罚项的特征选择法
使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。
- 基于树模型的特征选择
树模型中GBDT可用来作为基模型进行特征选择。
## 实践
对上面的特征选择方法进行了尝试，并对模型融合了解了一下